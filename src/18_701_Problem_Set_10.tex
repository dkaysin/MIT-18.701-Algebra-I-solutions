\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage[]{amsthm}
\usepackage[]{amssymb} 
\usepackage{mathrsfs}
\usepackage{tcolorbox}
\usepackage{nicefrac}
\usepackage{mathtools}
% \usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{array}

\graphicspath{ {./images/} }

\theoremstyle{definition}
\newtheorem*{claim}{Claim}
\newtheorem*{corollary}{Corollary}
\DeclareMathOperator{\adj}{\operatorname{adj}}
\DeclareMathOperator{\im}{\operatorname{im}}
\DeclareMathOperator{\spn}{\operatorname{span}}
\DeclareMathOperator{\nll}{\operatorname{null}}
\newcommand{\trace}{\operatorname{trace}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\operatorname{D}}
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\SL}{\operatorname{SL}}
\newcommand{\GLnR}{\GL_n(\R)}
\newcommand{\SLnR}{\SL_n(\R)}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\set{\{}{\}}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\genby{\langle}{\rangle}
\DeclarePairedDelimiter\bilform{\langle}{\rangle}
\newcommand{\restrict}[1]{ \big|_{#1} }
\newcommand{\evalat}[2]{\Big|_{#1}^{#2}}
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

\title{18.701: Problem Set 10}
\author{Dmitry Kaysin}
\date{August 2020}
\begin{document}
\maketitle 


\subsection*{Problem 1}

\begin{tcolorbox}
a) Let $\SL_2$ be the special linear group of real matrices with determinant $1$.
Determine the possible eigenvalues $\lambda$ (real or complex) of the elements of $\SL_2$, and make a drawing showing the points $\lambda$ in the complex plane.
\end{tcolorbox}

\begin{figure}[h]
    \caption{Possible eigenvalues of $A \in \SL_2$ in the complex plane.}
    \centering
    \includegraphics[width=0.8\textwidth]{ps10p1}
\end{figure}

We start with $2 \times 2$ matrix of the form
\[
    A =
    \begin{pmatrix}
        a & b \\
        c & d
    \end{pmatrix}
\]
with $\det A = ad - bc = 1$.

Characteristic polynomial of $A$ is $t^2 - tr + 1$ where $r = \trace A = a+d$.
Eigenvalues of $A$ are thus
\[
    \lambda = \frac{r \pm \sqrt{r^2-4}}{2}.
\]

For $r$ in the interval $(-\infty, -2] \cup [2, \infty)$ eigenvalues of $A$ are real and occur in pairs $\lambda, \frac{1}{\lambda}$ since the product of eigenvalues of $A$ is the determinant of $A$, which is equal to $1$.
As $r \to \infty : \lambda \to \pm \infty$ and
as $r \to -\infty : \lambda \to \mp 0$.
Real eigenvalues of the elements $\SL_2$ can be any number in $\R^\times$.

For $r$ in the interval $(-2, 2)$ eigenvalues of $A$ are complex, occur in conjugate pairs and their locus is upper and lower half of the unit circle on the complex plane.
Complex eigenvalues of the elements of $\SL_2$ can be any number on the unit circle in the complex plane.


\begin{tcolorbox}
b) For each $\lambda$, decompose the set of matrices $P \in \SL_2$ with eigenvalue $\lambda$ into $\SL_2$-conjugacy classes.
\end{tcolorbox}

\paragraph{Case 1: $A$ has eigenvalues $\pm 1$.}
Matrix $A$ has a real eigenvalue $\lambda$ of multiplicity $2$ equal to $1$ or $-1$.
It also has either one or two distinct eigenvectors.
If $A$ has two distinct eigenvectors, it must be diagonalizable, thus it must be similar to
$
\pm
\begin{pmatrix}
    1 & 0 \\
    0 & 1
\end{pmatrix}
$.
This is the first $\SL_2$-conjugacy class for $\lambda = \pm 1$.

If $A$ has only one distinct eigenvector $v$, it must correspond to a shear transformation.
We can choose the basis $B' = \set{v,v^\perp}$ where $v^\perp$ is orthogonal to $v$ and both $v$ and $v^\perp$ are unit vectors in the standard basis.
Change of basis from the standard basis to $B'$ transforms $A$ via conjugation to one of
$
    A'_p =
    \begin{pmatrix}
        \pm 1 & a \\
        0 & \pm 1
    \end{pmatrix}
$
or
$
    A'_n =
    \begin{pmatrix}
        \pm 1 & -a \\
        0 & \pm 1
    \end{pmatrix}
$.
We notice that 
$
    \begin{pmatrix}
        \pm 1 & a \\
        0 & \pm 1
    \end{pmatrix}
$
is similar to
$
    \begin{pmatrix}
        \pm 1 & 1 \\
        0 & \pm 1
    \end{pmatrix}
$
via conjugation by matrix
$
    \begin{pmatrix}
        a^{1/2} & a^{-1/2} \\
        0 & a^{-1/2}
    \end{pmatrix}
$,
which is an element of $\SL_2$.

We note that matrices $A'_p$ and $A'_n$ are not similar to one another since they are similar via conjugation by
$
    \begin{pmatrix}
        1 & 0 \\
        0 & -1
    \end{pmatrix}
$,
which has determinant $-1$ and thus is not an element of $\SL_2$.
This provides two more $\SL_2$-conjugacy classes for $\lambda = \pm 1$.

\paragraph{Case 2: $A$ has real eigenvalues $\set*{ \lambda, \frac{1}{\lambda} }$, $\lambda \neq 0, \> \lambda \neq \pm 1$.}
Real eigenvalues of $A$ are distinct, thus $A$ can be diagonalized via conjugation by invertible matrix $S$ whose columns are coordinates of the eigenvectors of $A$:
\[
    S A S^{-1} = 
    \begin{pmatrix}
        \lambda & \\
        & \nicefrac{1}{\lambda}
    \end{pmatrix}.
\]
We note that we can always choose $S$ so that it is an element of $\SL_2$.
We conclude that there is one $\SL_2$-conjugacy class for a given pair of real eigenvalues of $A$.

\paragraph{Case 3: $A$ has complex eigenvalues $\lambda$ and $\overline{\lambda}$.}

Consider 
\[
    v
    =
    \begin{pmatrix}
        x_1 + y_1 i \\
        x_2 + y_2 i
    \end{pmatrix}
    = X + iY,
\] 
a complex eigenvector of $A$.
Consider map $\varphi : \C^2 \to \GL_2(\R)$:
\[
    \begin{pmatrix}
        x_1 + y_1 i \\
        x_2 + y_2 i
    \end{pmatrix}
    \mapsto
    \begin{pmatrix}
        x_1 & y_1 \\
        x_2 & y_2
    \end{pmatrix}.
\]

Suppose $\det \varphi(v) = d$, then we can scale $v$ by $\nicefrac{1}{c^2}$ so that the resulting eigenvector has determinant $\pm 1$.
Vector $\overline{v} = X - iY$ is also an eigenvector of $A$ and $\det \varphi(\overline{v}) = -d$.

We choose eigenvector $v$ such that $\det \varphi (v) = 1$.
Suppose chosen $v$ has eigenvalue $\lambda$.
For such $v$ we have:
\begin{equation}
    \begin{aligned} \label{eq:eigv}
        A v 
        & = (a + bi) (X + iY)
        = aX + iaY + ibX - bY \\
        & = (aX-bY) + i (bX+aY)
        =
        \begin{pmatrix}
            a x_1 - b y_1 \\
            a x_2 - b y_2
        \end{pmatrix}
        +i
        \begin{pmatrix}
            b x_1 + a y_1 \\
            b x_2 + a y_2
        \end{pmatrix}.
    \end{aligned}
\end{equation}

Applying $\varphi$ to both sides of the equation (\ref{eq:eigv}) we have:
\[
    \varphi
    \left( A
        \begin{pmatrix}
            x_1 + y_1 i \\
            x_2 + y_2 i
        \end{pmatrix}
    \right)
    =
    \varphi
    \left(
        \begin{pmatrix}
            a x_1 - b y_1 \\
            a x_2 - b y_2
        \end{pmatrix}
        +i
        \begin{pmatrix}
            b x_1 + a y_1 \\
            b x_2 + a y_2
        \end{pmatrix}
    \right)
\]
We notice that $\varphi (Sv) = S \varphi(v)$ for any $2 \times 2$ real matrix $S$, therefore:
\[
    A T
    =
    \begin{pmatrix}
        a x_1 - b y_1 &  b x_1 + a y_1 \\
        a x_2 - b y_2 &  b x_2 + a y_2
    \end{pmatrix}
    =
    T
    \begin{pmatrix}
        a & b \\
        -b & a
    \end{pmatrix},
\]
where 
$
    T =
    \begin{pmatrix}
        x_1 & y_1 \\
        x_2 & y_2
    \end{pmatrix}
$.
From this we have:
\begin{align*}
    A T = T R_{(-\theta)}, \\
    T^{-1} A T = R_{(-\theta)},
\end{align*}
where $R_{(-\theta)}$ is the matrix of rotation by $-\theta$, which corresponds to multiplication by complex number $\overline{\lambda}$ of absolute value $1$.

Matrix $R_{(-\theta)}$ is an element of $\SL_2$ and we have specifically chosen $v$ such that $\det T = 1$, thus matrix $A$ is in the $\SL_2$-conjugacy class of $\R_{(-\theta)}$.

If we were to use eigenvector $\overline{v}$ with eigenvalue $\overline{\lambda}$, we would have arrived at the inverse of $R_{(-\theta)}$, matrix of rotation by $\theta$, which corresponds to multiplication by complex number $\lambda$.

Therefore, there are two $\SL_2$-conjugacy classes for each given complex conjugate pair of eigenvalues of $A$. 


\begin{tcolorbox}
c) Determine the matrices $P \in \SL_2$ that can be obtained as $P = e^A$ for some real matrix $A$.
\end{tcolorbox}

[TODO]


\subsection*{Problem 2}

\begin{tcolorbox}
According to Sylvester's Law, every $2 \times 2$ real symmetric matrix is congruent to exactly one of six standard types.
List them.
If we consider the operation of $\GL_2$ on $2 \times 2$ matrices by $P * A = PAP^t$, then Sylvester's Law asserts that the symmetric matrices form six orbits.
We may view the symmetric matrices as points in $\R^3$, letting $(x,y,z)$ correspond to the matrix
$
\begin{pmatrix}
    x & y \\
    y & z
\end{pmatrix}
$.
Describe the decomposition of $\R^3$ into orbits geometrically, and make a clear drawing depicting it.
\end{tcolorbox}

According to the Sylvester's Law, every symmetric $2 \times 2$ matrix is congruent to one of the following six "signature" matrices (unique up to permutation of basis vectors / diagonal entries):
\begin{gather*}
	I_{(2,0,0)} = 
	\begin{pmatrix}
		1 &  \\
		 & 1
	\end{pmatrix},
	\quad
	I_{(1,0,1)} = 
	\begin{pmatrix}
		1 &  \\
		 & 0
	\end{pmatrix}
	\sim
	\begin{pmatrix}
		0 &  \\
		 & 1
	\end{pmatrix},
	\\
	I_{(0,2,0)} =
	\begin{pmatrix}
		-1 &  \\
		 & -1
	\end{pmatrix},
	\quad
	I_{(0,1,1)} = 
	\begin{pmatrix}
		-1 &  \\
		 & 0
	\end{pmatrix}
	\sim
	\begin{pmatrix}
		0 &  \\
		 & -1
	\end{pmatrix},
	\\
	I_{(0,0,2)} =
	\begin{pmatrix}
		0 & 0 \\
		0 & 0
	\end{pmatrix},
	\quad
	I_{(1,1,0)} =
	\begin{pmatrix}
		1 &  \\
		 & -1
	\end{pmatrix}
	\sim
	\begin{pmatrix}
		-1 &  \\
		 & 1
	\end{pmatrix}.
\end{gather*}

We also note that since $\GL_2$ is connected, orbit of each "signature" matrix must also be connected.
Since no pair of "signature" matrices is connected, orbits of "signature" matrices are disjoint.
Moreover, orbits of the "signature" matrices partition the space of $2 \times 2$ matrices and $\R^3$.

We now examine orbits in $\R^3$ of six "signature" matrices.
Consider arbitrary matrix $P$ in $\GL_2$ with $\det P \neq 0$:
\[
	\begin{pmatrix}
		a & b \\
		c & d
	\end{pmatrix},
	\quad ad-bc \neq 0.
\]

\paragraph{Case 1: $I_{(2,0,0)}$}

\begin{align*}
	P * I_{(2,0,0)} & =
	P I_{(2,0,0)} P^t =
	\begin{pmatrix}
		a & b \\
		c & d
	\end{pmatrix}
	\begin{pmatrix}
		1 & 0 \\
		0 & 1
	\end{pmatrix}
	\begin{pmatrix}
		a & c \\
		b & d
	\end{pmatrix}
	=
	\begin{pmatrix}
		a^2+b^2 & ac+bd \\
		ac+bd & a^2+b^2
	\end{pmatrix}
	\\
	& = (a^2+b^2, ac+bd, a^2+b^2)
	= (v_1 \cdot v_1, v_1 \cdot v_2, v_2 \cdot v_2)
	\\
	& = (\abs{v_1}^2, \abs{v_1}\abs{v_2}\cos\phi, \abs{v_1}^2),
\end{align*}
where $v_1 = (a,b)^t$, $v_2 = (c,d)^t$ and $\phi$ is an oriented angle between $v_1$ and $v_2$.
We note that $v_1$ and $v_2$ must be nonzero and must not be collinear, otherwise determinant of $P$ would be equal to $0$; therefore $v_1 \neq 0, v_2 \neq 0$ and $\phi \in (0,\pi) \cup (\pi, 2\pi)$.

Since $\abs{v_1}$ and $\abs{v_2}$ can be any positive real number, the orbit of $I_{(2,0,0)}$ under conjugation by an element of $\GL_2$ is a subset of $\R^3$
\[
	\GL_2 (I_{(2,0,0)}) = \set{(r^2, \lambda rs, s^2): r,s \in (0, \infty), \lambda \in (-1,1)}.
\]
By inspection, the orbit is an interior of a positive cone. 

\paragraph{Case 2: $I_{(1,0,1)}$}

\begin{align*}
	P * I_{(1,0,1)} & =
	P I_{(1,0,1)} P^t =
	\begin{pmatrix}
		a & b \\
		c & d
	\end{pmatrix}
	\begin{pmatrix}
		1 & 0 \\
		0 & 0
	\end{pmatrix}
	\begin{pmatrix}
		a & c \\
		b & d
	\end{pmatrix}
	=
	\begin{pmatrix}
		a^2 & ac \\
		ac & c^2
	\end{pmatrix}
	\\
	& = (a^2, ac, c^2).
\end{align*}

Therefore the orbit of $I_{(1,1,0)}$ under conjugation by an element of $\GL_2$ is a subset of $\R^3$
\[
	\GL_2 (I_{(1,1,0)}) = \set{(a^2, ac, c^2): a,c \in \R^\times}.
\]
By inspection, the orbit is a positive cone with the vertex removed. 

\begin{figure}[h]
    \caption{Cones that are the orbits of $I_{(1,0,1)}$ and $I_{(0,1,1)}$. Orbits of $I_{(2,0,0)}$ and $I_{(0,2,0)}$ are interior of these cones. Zero point is removed.}
    \includegraphics[width=0.8 \textwidth]{Both.png}
    % \begin{subfigure}{0.5\textwidth}
    %     \includegraphics[width=\linewidth]{(1,0,1).png}
    % \end{subfigure}
    % \begin{subfigure}{0.5\textwidth}
    %     \includegraphics[width=\linewidth]{(0,1,1).png}
    % \end{subfigure}
\end{figure}

\paragraph{Case 3: $I_{(0,2,0)}$}

\[
	P * I_{(0,2,0)} = P I_{(0,2,0)} P^t = - P I_{(2,0,0)} P^t.
\]
From this we can see that the orbit of $I_{(0,2,0)}$ under conjugation is
\[
	\set{(-r^2, \lambda rs, -s^2) \> : \> r,s \in (0, \infty), \lambda \in (-1,1)},
\]
which is an interior of a negative cone. 

\paragraph{Case 4: $I_{(0,1,1)}$}

\[
    P * I_{(,1,1)} = P I_{(0,1,1)} P^t = - P I_{(1,0,1)} P^t.
\]
Therefore the orbit of $I_{(0,1,1)}$ under conjugation is
\[
	\GL_2 (I_{(0,1,1)}) = \set{(-a^2, ac, -c^2): a,c \in \R^\times},
\]
which is a negative cone with the vertex removed. 

\paragraph{Case 5: $I_{(0,0,2)}$}

\[
	P * I_{(0,0,2)} = P I_{(0,0,2)} P^t = I_{(0,0,2)}.
\]
The orbit of $I_{(0,0,2)}$ under conjugation is $I_{(0,0,2)} = (0,0,0)$.

\paragraph{Case 6: $I_{(1,1,0)}$}

Since orbits of "signature" matrices partition $\R^3$, the only remaining subset of $\R^3$ is the exterior of a double cone.

Curiously, the orbit of $I_{(1,1,0)}$ can be parameterized via a function $\varphi : \C \times \C \to \R^3$:
\begin{equation} \label{eq:case6}
    \varphi(z_1, z_2) =
    \begin{pmatrix}
        \Re (z_1^2) \\
        \Re (z_1 z_2) \\
        \Re (z_2^2)
    \end{pmatrix}
\end{equation}

\begin{proof}

\begin{align*}
	P * I_{(1,1,0)} & =
	P I_{(1,1,0)} P^t =
	\begin{pmatrix}
		a & b \\
		c & d
	\end{pmatrix}
	\begin{pmatrix}
		1 & 0 \\
		0 & -1
	\end{pmatrix}
	\begin{pmatrix}
		a & c \\
		b & d
	\end{pmatrix}
	=
	\begin{pmatrix}
		a & b \\
		c & d
	\end{pmatrix}
	\begin{pmatrix}
		a & c \\
		-b & -d
	\end{pmatrix}
	\\
	& =
	\begin{pmatrix}
		a^2-b^2 & ac-bd \\
		ac-bd & c^2-d^2
	\end{pmatrix}
	=
	\begin{pmatrix}
    	a^2-b^2 \\
    	ac-bd \\
    	c^2-d^2
	\end{pmatrix},
\end{align*}
which produces an expression equivalent to (\ref{eq:case6}) for complex numbers $z_1 = a+bi, z_2 = c+di$.

\end{proof}


\subsection*{Problem 3}

\begin{tcolorbox}
This problem is about the space $V$ of real polynomials in the variables $x$ and $y$.
If $f$ is a polynomial, $\partial_f$ will denote the operator 
$f \left( \frac{\partial}{\partial x}, \frac{\partial}{\partial y} \right)$,
and $\partial_f(g)$ will denote the result of applying this operator to a polynomial $g$.

a) The rule $\bilform{f,g} = \partial_f(g)_0$ defines a bilinear form on $V$, the subscript denoting evaluation of a polynomial at the origin.
Prove that this form is symmetric and positive definite, and that the monomials $x^i y^j$ form an orthogonal basis of $V$.
\end{tcolorbox}

\begin{proof}

Let $f = x^i y^j, g = x^k y^n$.

If $j > n$ then
\[
	\bilform{f,g} 
	= D_x^i D_y^j x^k y^n 
	= n! \> D_x^i D_y^{j-n} x^k
	= 0.
\]
Similarly, if $i > k$, we have $\bilform{f,g} = 0$.

If $j < n$ while $i \leq k$ then
\[
	\bilform{f,g} 
	= D_x^i D_y^j x^k y^n 
	= n (n-1) \cdots (n-j+1) \cdot k (k-1) \cdots (k-i+1) \cdot x^{k-i} y^{n-j},
\]
which evaluates to $0$ at the origin.
Similarly, if $i < k$ while $j \leq n$ , we have $\bilform{f,g} = 0$.

Finally, if $i = k, j = n$, we have
\[
	\bilform{f,g}
	= D_x^i D_y^j x^i y^j
	= (i!) (j!)
\]
It is now easy to see that $\bilform{f,g} = \bilform{g,f}$ for any monomials $f$ and $g$.
Since monomials form a basis of $V$, this result carries over to an arbitrary polynomial.
This proves that $\bilform{\cdot,\cdot}$ is symmetric, as required.

Moreover, as we have seen previously, monomials $f = x^i y^j$ and $g = x^k y^n$ are orthogonal with respect to $\bilform{\cdot,\cdot}$ unless $i = k, j = n$.
This proves that the basis of monomials is orthogonal, as required.

For nonzero $f = \sum_i \sum_j a_{ij} \> x^i y^j$ we have:
\[
	\bilform{f,f}
	= \sum_i (i!)^2,
\]
which is always positive since $i > 0$.
We conclude that $\bilform{\cdot,\cdot}$ is positive definite, as required.

\end{proof}


\begin{tcolorbox}
b) We also have the operator of multiplication by $q$, which we write as $m_q$.
So $m_q(g) = qg$.
Prove that $\partial_q$ and $m_q$ are adjoint operators.
\end{tcolorbox}

\begin{proof}

Operators $\partial_q$ and $m_q$ are adjoint if
\begin{equation} \label{eq:adjoint}
    \bilform{\partial_q f, g} = \bilform{f, m_q g} \text{ for all $f, g$}.
\end{equation}

Consider monomials $f = x^i y^j, g = x^k y^n$ and $q = x^r y^s$.

\[
    \partial_q (f)
    = D_x^r D_y^s x^i y^j
    = \frac{i!}{(i-r)!} \frac{j!}{(j-s)!} x^{i-r} y^{j-s},
\]
\[
    \bilform{\partial_q (f), g} 
    = \frac{i!}{(i-r)!} \frac{j!}{(j-s)!} \bilform{x^{i-r} y^{j-s}, x^k y^n}
\]

\paragraph{Case 1: $i-r = k$ and $j-s = n$.}
Evaluating the left-hand side of (\ref{eq:adjoint}) we have:
\[
    \bilform{\partial_q (f), g} 
    = \frac{i!}{k!} \frac{j!}{n!} \bilform{x^k y^n, x^k y^n}
    = \frac{i!}{k!} \frac{j!}{n!} \> k! \> n! 
    = (i!) (j!).
\]
Evaluating the right-hand side of (\ref{eq:adjoint}) we have:
\[
    \bilform{f, m_q(g)} 
    = \bilform{x^i y^j, x^{k+r} y^{n+s}}
    = \bilform{x^i y^j, x^i y^j}
    = (i!) (j!).
\]
Therefore, $\bilform{\partial_q (f), g} = \bilform{f, m_q(g)}$

\paragraph{Case 2: $i-r \neq k$ or $j-s \neq n$.}
Evaluating the left-hand side of (\ref{eq:adjoint}) we have:
\[
    \bilform{\partial_q (f), g} 
    = \frac{i!}{(i-r)!} \frac{j!}{(j-s)!} \bilform{x^{i-r} y^{j-s}, x^k y^n}
    = 0.
\]
Evaluating the right-hand side of (\ref{eq:adjoint}) we have:
\[
    \bilform{f, m_q(g)} 
    = \bilform{x^i y^j, x^{k+r} y^{n+s}}
    = 0.
\]
We conclude that $\partial_q$ and $m_q$ are adjoint operators.

\end{proof}


\begin{tcolorbox}
c) When $f = x^2 + y^2$, the operator $\partial_f$ is the Laplacian, which is often written as $\Delta$.
A polynomial $h$ is harmonic if $\Delta h = 0$.
Let $H$ denote the space of harmonic polynomials.
Identify the space $H^\perp$ orthogonal to $H$ with respect to the given form.
\end{tcolorbox}

Consider arbitrary $h \in H$ and $g \in V$.
Since $\partial_f$ and $m_f$ are adjoint operators, we have:
\begin{gather*}
	\bilform{\partial_f(h), g} = \bilform{h, m_f(g)}, \\
	\intertext{by definition of $H$ we have $\partial_f(h) = 0$, thus:}
	\bilform{0, g} = \bilform{h, fg}.
\end{gather*}
Since $\bilform{0, g} = 0$ for any $g$, we have $\bilform{h, fg} = 0$.
Therefore, $h$ and $fg$ are orthogonal.
Since $g$ can be arbitrary, harmonic polynomials are orthogonal to any polynomial divisible by $f = x^2+y^2$.
Denote $W$ the space of polynomials that are divisible by $f$.

To prove that any polynomial that is orthogonal to harmonic polynomials is in $W$, we need to show that $W \oplus H = V$.
We have already established that $H \cap W = 0$ since $H$ and $W$ are orthogonal, therefore it suffices to show that $H + W = V$, i.e. any polynomial $p \in V$ can be represented as a linear combination of a harmonic polynomial and a polynomial divisible by $f$.

Consider arbitrary monomial $v = x^a y^b$.
It can be represented as a sum of a polynomial divisible by $f$ and a harnomic polynomial if and only if there exists polynomial $g$ such that $\partial_f (v - fg) = 0$.
Solving this equation for $g$ we have:
\[
	g(a,b) = \frac{a(a-1) x^{a-2} y^b}{4} + \frac{b(b-1) x^a y^{b-2}}{4},
\]
which is a well-defined polynomial in $V$.
Therefore, any monomial $v = x^a y^b$ can be pepresented as a sum of a polynomial divisible by $f$, namely $fg(a,b)$, and a harmonic polynomial $v - fg(a,b)$.
The result carries over to any arbitrary polynomial $p \in V$, which is a linear combination of monomials.

The Matlab code that was used to find $g(a,b)$ is provided below: 
\begin{verbatim}
    syms x y a b g;
    v = x^a * y^b;
    f = x^2 + y^2;
    eq = laplacian(f*g, [x,y]) == laplacian(v, [x,y]);
    solve(eq, g)
\end{verbatim}


\subsection*{Problem 4}

\begin{tcolorbox}
Show that the vector cross product makes $\R^3$ into a Lie algebra $L_1$.
\end{tcolorbox}

\begin{proof}
\end{proof}


\end{document}
